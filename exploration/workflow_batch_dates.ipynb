{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Date Expected Proceeds Prediction Workflow\n",
    "\n",
    "This notebook runs the expected proceeds prediction workflow for multiple inference dates and combines the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Import custom modules\n",
    "from country_utils import add_signup_country_group\n",
    "from data_splitter_utils import split_data_by_date, split_data_by_user_type\n",
    "from trial_predictions import TrialPredictionModel\n",
    "from direct_purchase_predictions import DirectPurchasePredictionModel\n",
    "from lag_purchase_predictions import LagPurchasePredictionModel\n",
    "from prediction_processor import PredictionProcessor\n",
    "\n",
    "# Connect to Snowflake\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Inference Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of inference dates\n",
    "# Format: 'YYYY-MM-DD'\n",
    "inference_dates = [\n",
    "    '2023-01-01',\n",
    "    '2023-02-01',\n",
    "    '2023-03-01',\n",
    "    '2023-04-01',\n",
    "    '2023-05-01'\n",
    "]\n",
    "\n",
    "# Alternatively, generate dates in a range\n",
    "def generate_date_range(start_date, end_date, interval_days=30):\n",
    "    \"\"\"Generate a list of dates between start_date and end_date with specified interval\"\"\"\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    dates = []\n",
    "    current = start\n",
    "    \n",
    "    while current <= end:\n",
    "        dates.append(current.strftime('%Y-%m-%d'))\n",
    "        current += timedelta(days=interval_days)\n",
    "    \n",
    "    return dates\n",
    "\n",
    "# Uncomment to use date range instead of explicit list\n",
    "# inference_dates = generate_date_range('2023-01-01', '2023-05-01', 30)\n",
    "\n",
    "print(f\"Will process {len(inference_dates)} inference dates:\")\n",
    "for date in inference_dates:\n",
    "    print(f\"  - {date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from Snowflake\n",
    "# Get data for a wide enough range to cover all inference dates\n",
    "earliest_date = min(inference_dates)\n",
    "latest_date = max(inference_dates)\n",
    "\n",
    "# Calculate the training window needed\n",
    "training_window_days = 180  # Use 1/2 year of training data\n",
    "\n",
    "# Calculate the earliest date needed for training\n",
    "earliest_training_date = (datetime.strptime(earliest_date, '%Y-%m-%d') - \n",
    "                          timedelta(days=training_window_days)).strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Fetching data from {earliest_training_date} to {latest_date}\")\n",
    "\n",
    "input_query = f\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM blinkist_dev.dbt_mjaama.exp_proceeds_input\n",
    "    WHERE report_date >= '{earliest_training_date}'\n",
    "    AND report_date <= '{latest_date}'\n",
    "    \"\"\"\n",
    "    \n",
    "input_df = session.sql(input_query).to_pandas()\n",
    "\n",
    "# Load product dimension data\n",
    "product_query = \"\"\"\n",
    "    select sku as product_name, price \n",
    "    from BLINKIST_PRODUCTION.reference_tables.product_dim\n",
    "    where is_purchasable;\n",
    "    \"\"\"\n",
    "    \n",
    "product_df = session.sql(product_query).to_pandas()\n",
    "\n",
    "# Display data info\n",
    "print(f\"Input data shape: {input_df.shape}\")\n",
    "print(f\"Product data shape: {product_df.shape}\")\n",
    "\n",
    "# Add signup_country_group to the input data\n",
    "input_df = add_signup_country_group(input_df)\n",
    "\n",
    "# Check the distribution of country groups\n",
    "country_group_counts = input_df['signup_country_group'].value_counts()\n",
    "print(\"\\nCountry group distribution:\")\n",
    "print(country_group_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction_for_date(input_data, product_data, inference_date, training_window_days=180):\n",
    "    \"\"\"Run the full prediction workflow for a specific inference date\n",
    "    \n",
    "    Args:\n",
    "        input_data: DataFrame with input data\n",
    "        product_data: DataFrame with product dimension data\n",
    "        inference_date: Date string in format 'YYYY-MM-DD'\n",
    "        training_window_days: Number of days to use for training\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions for the inference date\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing inference date: {inference_date}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 1. Split data by date\n",
    "    inference_df, training_d8_df, training_d100_df = split_data_by_date(\n",
    "        input_data,\n",
    "        inference_date=inference_date,\n",
    "        training_window_days=training_window_days,\n",
    "        date_column='report_date'\n",
    "    )\n",
    "    \n",
    "    print(f\"Inference data shape: {inference_df.shape}\")\n",
    "    print(f\"Training d8 data shape: {training_d8_df.shape}\")\n",
    "    print(f\"Training d100 data shape: {training_d100_df.shape}\")\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if inference_df.empty or training_d8_df.empty or training_d100_df.empty:\n",
    "        print(f\"Insufficient data for inference date {inference_date}. Skipping.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # 2. Split data by user type\n",
    "    trial_inference, day0_payers_inference, other_inference = split_data_by_user_type(inference_df)\n",
    "    trial_training_d8, day0_payers_training_d8, other_training_d8 = split_data_by_user_type(training_d8_df)\n",
    "    trial_training_d100, day0_payers_training_d100, other_training_d100 = split_data_by_user_type(training_d100_df)\n",
    "    \n",
    "    print(\"Inference data user type distribution:\")\n",
    "    print(f\"Trial users: {trial_inference.shape[0]}\")\n",
    "    print(f\"Day 0 payers: {day0_payers_inference.shape[0]}\")\n",
    "    print(f\"Other users: {other_inference.shape[0]}\")\n",
    "    \n",
    "    # 3. Train models\n",
    "    print(\"\\nTraining models...\")\n",
    "    \n",
    "    # Trial model\n",
    "    trial_model = None\n",
    "    if not trial_training_d8.empty and not trial_training_d100.empty:\n",
    "        trial_model = TrialPredictionModel(product_dim_df=product_data)\n",
    "        trial_model.fit(trial_training_d8, trial_training_d100)\n",
    "        print(\"  - Trial model trained successfully\")\n",
    "    else:\n",
    "        print(\"  - Insufficient data for trial model\")\n",
    "    \n",
    "    # Direct purchase model\n",
    "    direct_model = None\n",
    "    if not day0_payers_training_d8.empty and not day0_payers_training_d100.empty:\n",
    "        direct_model = DirectPurchasePredictionModel()\n",
    "        direct_model.fit(day0_payers_training_d8, day0_payers_training_d100)\n",
    "        print(\"  - Direct purchase model trained successfully\")\n",
    "    else:\n",
    "        print(\"  - Insufficient data for direct purchase model\")\n",
    "    \n",
    "    # Lag purchase model\n",
    "    lag_model = None\n",
    "    if not other_training_d8.empty and not other_training_d100.empty:\n",
    "        lag_model = LagPurchasePredictionModel(product_dim_df=product_data)\n",
    "        lag_model.fit(other_training_d8, other_training_d100)\n",
    "        print(\"  - Lag purchase model trained successfully\")\n",
    "    else:\n",
    "        print(\"  - Insufficient data for lag purchase model\")\n",
    "    \n",
    "    # 4. Make predictions\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    predictions = []\n",
    "    \n",
    "    # Trial users\n",
    "    if trial_model is not None and not trial_inference.empty:\n",
    "        trial_predictions = trial_model.predict(trial_inference)\n",
    "        predictions.append(trial_predictions)\n",
    "        print(f\"  - Predicted for {trial_predictions.shape[0]} trial users\")\n",
    "    \n",
    "    # Day 0 payers\n",
    "    if direct_model is not None and not day0_payers_inference.empty:\n",
    "        direct_predictions = direct_model.predict(day0_payers_inference)\n",
    "        predictions.append(direct_predictions)\n",
    "        print(f\"  - Predicted for {direct_predictions.shape[0]} day 0 payers\")\n",
    "    \n",
    "    # Other users\n",
    "    if lag_model is not None and not other_inference.empty:\n",
    "        lag_predictions = lag_model.predict(other_inference)\n",
    "        predictions.append(lag_predictions)\n",
    "        print(f\"  - Predicted for {lag_predictions.shape[0]} other users\")\n",
    "    \n",
    "    return  pd.concat(predictions, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Predictions for All Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store results\n",
    "all_results = []\n",
    "\n",
    "# Track timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each inference date\n",
    "for date in inference_dates:\n",
    "    date_start_time = time.time()\n",
    "    \n",
    "    # Run prediction for this date\n",
    "    result_df = run_prediction_for_date(\n",
    "        input_data=input_df,\n",
    "        product_data=product_df,\n",
    "        inference_date=date,\n",
    "        training_window_days=training_window_days\n",
    "    )\n",
    "    \n",
    "    # Add to results if not empty\n",
    "    if not result_df.empty:\n",
    "        all_results.append(result_df)\n",
    "        \n",
    "    date_end_time = time.time()\n",
    "    print(f\"Processed {date} in {date_end_time - date_start_time:.2f} seconds\")\n",
    "\n",
    "# Combine all results\n",
    "if all_results:\n",
    "    combined_results = pd.concat(all_results, ignore_index=False)\n",
    "    print(f\"\\nCombined results shape: {combined_results.shape}\")\n",
    "else:\n",
    "    combined_results = pd.DataFrame()\n",
    "    print(\"\\nNo results generated.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total processing time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_results.empty:\n",
    "    # Initialize the prediction processor\n",
    "    processor = PredictionProcessor()\n",
    "    \n",
    "    # Analyze by inference date\n",
    "    date_performance = processor.aggregate_predictions(\n",
    "        combined_results, \n",
    "        custom_grouping=['inference_date']\n",
    "    )\n",
    "    \n",
    "    print(\"Performance by Inference Date:\")\n",
    "    display(date_performance.sort_values('inference_date'))\n",
    "    \n",
    "    # Analyze by channel and inference date\n",
    "    channel_date_performance = processor.aggregate_predictions(\n",
    "        combined_results, \n",
    "        custom_grouping=['inference_date', 'channel_group']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPerformance by Channel and Inference Date:\")\n",
    "    display(channel_date_performance.sort_values(['inference_date', 'expected_proceeds_d100'], ascending=[True, False]))\n",
    "    \n",
    "    # Analyze by country group and inference date\n",
    "    country_date_performance = processor.aggregate_predictions(\n",
    "        combined_results, \n",
    "        custom_grouping=['inference_date', 'signup_country_group']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPerformance by Country Group and Inference Date:\")\n",
    "    display(country_date_performance.sort_values(['inference_date', 'expected_proceeds_d100'], ascending=[True, False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined results to Snowflake\n",
    "if not combined_results.empty:\n",
    "\n",
    "    session = get_snowflake_session()\n",
    "    # Filter columns that exist in the DataFrame\n",
    "\n",
    "    \n",
    "    # Convert to Snowpark DataFrame\n",
    "    snowpark_df = session.create_dataframe(combined_results)\n",
    "    \n",
    "    # Save to Snowflake table\n",
    "    table_name = \"BLINKIST_DEV.DBT_MJAAMA.MULTI_DATE_EXPECTED_PROCEEDS_PREDICTIONS\"\n",
    "    snowpark_df.write.mode(\"overwrite\").save_as_table(table_name)\n",
    "    \n",
    "    print(f\"Combined predictions saved to {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results to CSV (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export combined results to CSV\n",
    "if not combined_results.empty:\n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"multi_date_predictions_{timestamp}.csv\"\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_results.to_csv(filename, index=False)\n",
    "    print(f\"Combined predictions exported to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
