{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Proceeds Prediction Workflow\n",
    "\n",
    "This notebook demonstrates the end-to-end workflow for predicting expected proceeds at day 8 and day 100 for different user types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom modules\n",
    "from country_utils import add_signup_country_group\n",
    "from data_utils import split_data_by_date, split_data_by_user_type\n",
    "from trial_predictions import TrialPredictionModel\n",
    "from direct_purchase_predictions import DirectPurchasePredictionModel\n",
    "from lag_purchase_predictions import LagPurchasePredictionModel\n",
    "from prediction_processor import PredictionProcessor\n",
    "\n",
    "# Connect to Snowflake\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from Snowflake\n",
    "input_query = \"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM blinkist_dev.dbt_mjaama.exp_proceeds_input\n",
    "    \"\"\"\n",
    "    \n",
    "input_df = session.sql(input_query).to_pandas()\n",
    "\n",
    "# Load product dimension data\n",
    "product_query = \"\"\"\n",
    "    select sku as product_name, price \n",
    "    from BLINKIST_PRODUCTION.reference_tables.product_dim\n",
    "    where is_purchasable;\n",
    "    \"\"\"\n",
    "    \n",
    "product_df = session.sql(product_query).to_pandas()\n",
    "\n",
    "# Display data info\n",
    "print(f\"Input data shape: {input_df.shape}\")\n",
    "print(f\"Product data shape: {product_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Country Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add signup_country_group to the input data\n",
    "input_df = add_signup_country_group(input_df)\n",
    "\n",
    "# Check the distribution of country groups\n",
    "country_group_counts = input_df['signup_country_group'].value_counts()\n",
    "print(\"Country group distribution:\")\n",
    "print(country_group_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Data by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference date and training window\n",
    "inference_date = '2025-01-01'  # Using a future date for inference\n",
    "training_window_days = 180     # Use 1/2 year of training data\n",
    "\n",
    "# Split data into inference, training_d8, and training_d100 datasets\n",
    "inference_df, training_d8_df, training_d100_df = split_data_by_date(\n",
    "    input_df,\n",
    "    inference_date=inference_date,\n",
    "    training_window_days=training_window_days,\n",
    "    date_column='report_date'\n",
    ")\n",
    "\n",
    "print(f\"Inference data shape: {inference_df.shape}\")\n",
    "print(f\"Training d8 data shape: {training_d8_df.shape}\")\n",
    "print(f\"Training d100 data shape: {training_d100_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Data by User Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split inference data by user type\n",
    "trial_inference, day0_payers_inference, other_inference = split_data_by_user_type(inference_df)\n",
    "\n",
    "# Split training data for d8 predictions by user type\n",
    "trial_training_d8, day0_payers_training_d8, other_training_d8 = split_data_by_user_type(training_d8_df)\n",
    "\n",
    "# Split training data for d100 predictions by user type\n",
    "trial_training_d100, day0_payers_training_d100, other_training_d100 = split_data_by_user_type(training_d100_df)\n",
    "\n",
    "# Display user type distribution\n",
    "print(\"Inference data user type distribution:\")\n",
    "print(f\"Trial users: {trial_inference.shape[0]}\")\n",
    "print(f\"Day 0 payers: {day0_payers_inference.shape[0]}\")\n",
    "print(f\"Other users: {other_inference.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train Trial Prediction Model\n",
    "print(\"Training Trial Prediction Model...\")\n",
    "trial_model = TrialPredictionModel(product_dim_df=product_df)\n",
    "trial_model.fit(trial_training_d8, trial_training_d100)\n",
    "\n",
    "# 2. Train Direct Purchase Prediction Model\n",
    "print(\"Training Direct Purchase Prediction Model...\")\n",
    "direct_model = DirectPurchasePredictionModel()\n",
    "direct_model.fit(day0_payers_training_d8, day0_payers_training_d100)\n",
    "\n",
    "# 3. Train Lag Purchase Prediction Model\n",
    "print(\"Training Lag Purchase Prediction Model...\")\n",
    "lag_model = LagPurchasePredictionModel(product_dim_df=product_df)\n",
    "lag_model.fit(other_training_d8, other_training_d100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for each user type\n",
    "predictions = []\n",
    "\n",
    "# 1. Trial users\n",
    "if not trial_inference.empty:\n",
    "    print(\"Predicting for trial users...\")\n",
    "    trial_predictions = trial_model.predict(trial_inference)\n",
    "    predictions.append(trial_predictions)\n",
    "\n",
    "# 2. Day 0 payers\n",
    "if not day0_payers_inference.empty:\n",
    "    print(\"Predicting for day 0 payers...\")\n",
    "    direct_predictions = direct_model.predict(day0_payers_inference)\n",
    "    predictions.append(direct_predictions)\n",
    "\n",
    "# 3. Other users\n",
    "if not other_inference.empty:\n",
    "    print(\"Predicting for other users...\")\n",
    "    lag_predictions = lag_model.predict(other_inference)\n",
    "    predictions.append(lag_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process and Aggregate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the prediction processor\n",
    "processor = PredictionProcessor()\n",
    "\n",
    "predictions_df = pd.concat(predictions, ignore_index=False)\n",
    "# Combine all predictions\n",
    "all_predictions = processor.process_predictions(predictions_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
